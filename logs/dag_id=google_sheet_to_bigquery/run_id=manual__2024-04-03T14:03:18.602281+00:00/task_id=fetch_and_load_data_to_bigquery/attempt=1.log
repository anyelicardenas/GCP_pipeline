[2024-04-03T14:03:22.800+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: google_sheet_to_bigquery.fetch_and_load_data_to_bigquery manual__2024-04-03T14:03:18.602281+00:00 [queued]>
[2024-04-03T14:03:22.816+0000] {taskinstance.py:1979} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: google_sheet_to_bigquery.fetch_and_load_data_to_bigquery manual__2024-04-03T14:03:18.602281+00:00 [queued]>
[2024-04-03T14:03:22.817+0000] {taskinstance.py:2193} INFO - Starting attempt 1 of 2
[2024-04-03T14:03:22.843+0000] {taskinstance.py:2217} INFO - Executing <Task(PythonOperator): fetch_and_load_data_to_bigquery> on 2024-04-03 14:03:18.602281+00:00
[2024-04-03T14:03:22.853+0000] {standard_task_runner.py:60} INFO - Started process 7672 to run task
[2024-04-03T14:03:22.859+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'google_sheet_to_bigquery', 'fetch_and_load_data_to_bigquery', 'manual__2024-04-03T14:03:18.602281+00:00', '--job-id', '118', '--raw', '--subdir', 'DAGS_FOLDER/dags/habidag.py', '--cfg-path', '/tmp/tmpfhl6lg62']
[2024-04-03T14:03:22.861+0000] {standard_task_runner.py:88} INFO - Job 118: Subtask fetch_and_load_data_to_bigquery
[2024-04-03T14:03:22.930+0000] {task_command.py:423} INFO - Running <TaskInstance: google_sheet_to_bigquery.fetch_and_load_data_to_bigquery manual__2024-04-03T14:03:18.602281+00:00 [running]> on host 84784c125ee0
[2024-04-03T14:03:23.064+0000] {taskinstance.py:2513} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='google_sheet_to_bigquery' AIRFLOW_CTX_TASK_ID='fetch_and_load_data_to_bigquery' AIRFLOW_CTX_EXECUTION_DATE='2024-04-03T14:03:18.602281+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-04-03T14:03:18.602281+00:00'
[2024-04-03T14:03:24.107+0000] {connection.py:269} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2024-04-03T14:03:24.114+0000] {base.py:83} INFO - Using connection ID 'google_cloud_default' for task execution.
[2024-04-03T14:03:24.115+0000] {bigquery.py:470} INFO - datasetId was not specified in `dataset_reference`. Will use default value datalake_anyeli8.
[2024-04-03T14:03:24.115+0000] {bigquery.py:470} INFO - projectId was not specified in `dataset_reference`. Will use default value papyrus-technical-test.
[2024-04-03T14:03:24.116+0000] {bigquery.py:481} INFO - Creating dataset: datalake_anyeli8 in project: papyrus-technical-test 
[2024-04-03T14:03:25.292+0000] {bigquery.py:485} INFO - Dataset created successfully.
[2024-04-03T14:03:25.677+0000] {connection.py:269} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2024-04-03T14:03:25.681+0000] {base.py:83} INFO - Using connection ID 'google_cloud_default' for task execution.
[2024-04-03T14:03:25.683+0000] {bigquery.py:1516} INFO - Creating table
[2024-04-03T14:03:26.916+0000] {bigquery.py:1538} INFO - Table papyrus-technical-test.datalake_anyeli8.transportistas created successfully
[2024-04-03T14:03:27.006+0000] {logging_mixin.py:188} INFO -    idTransportista     nombreEmpresa
0                1    Speedy Express
1                2    United Package
2                3  Federal Shipping
3                4     test cargue 1
[2024-04-03T14:03:27.023+0000] {connection.py:269} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2024-04-03T14:03:27.027+0000] {base.py:83} INFO - Using connection ID 'google_cloud_default' for task execution.
[2024-04-03T14:03:27.125+0000] {bigquery.py:2811} INFO - Executing: {'query': {'query': "INSERT `papyrus-technical-test.datalake_anyeli8.transportistas` VALUES (1, 'Speedy Express'), (2, 'United Package'), (3, 'Federal Shipping'), (4, 'test cargue 1');", 'useLegacySql': False, 'priority': 'BATCH'}}'
[2024-04-03T14:03:27.126+0000] {bigquery.py:1613} INFO - Inserting job ***_adhoc_***_insert_data_transportistas_2024_04_03T14_03_18_602281_00_00_ab974c8eec0b5cc73b858e8c6f808229
[2024-04-03T14:03:30.118+0000] {connection.py:269} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2024-04-03T14:03:30.122+0000] {base.py:83} INFO - Using connection ID 'google_cloud_default' for task execution.
[2024-04-03T14:03:30.123+0000] {bigquery.py:1516} INFO - Creating table
[2024-04-03T14:03:31.226+0000] {bigquery.py:1538} INFO - Table papyrus-technical-test.datalake_anyeli8.categorias created successfully
[2024-04-03T14:03:31.265+0000] {logging_mixin.py:188} INFO -    idCategoria  ...                                        descripcion
0            1  ...        Soft drinks, coffees, teas, beers, and ales
1            2  ...  Sweet and savory sauces, relishes, spreads, an...
2            3  ...                Desserts, candies, and sweet breads
3            4  ...                                            Cheeses
4            5  ...                Breads, crackers, pasta, and cereal
5            6  ...                                     Prepared meats
6            7  ...                          Dried fruit and bean curd
7            8  ...                                   Seaweed and fish

[8 rows x 3 columns]
[2024-04-03T14:03:31.275+0000] {connection.py:269} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2024-04-03T14:03:31.277+0000] {base.py:83} INFO - Using connection ID 'google_cloud_default' for task execution.
[2024-04-03T14:03:31.345+0000] {bigquery.py:2811} INFO - Executing: {'query': {'query': "INSERT `papyrus-technical-test.datalake_anyeli8.categorias` VALUES (1, 'Beverages', 'Soft drinks, coffees, teas, beers, and ales'), (2, 'Condiments', 'Sweet and savory sauces, relishes, spreads, and seasonings'), (3, 'Confections', 'Desserts, candies, and sweet breads'), (4, 'Dairy Products', 'Cheeses'), (5, 'Grains & Cereals', 'Breads, crackers, pasta, and cereal'), (6, 'Meat & Poultry', 'Prepared meats'), (7, 'Produce', 'Dried fruit and bean curd'), (8, 'Seafood', 'Seaweed and fish');", 'useLegacySql': False, 'priority': 'BATCH'}}'
[2024-04-03T14:03:31.346+0000] {bigquery.py:1613} INFO - Inserting job ***_adhoc_***_insert_data_categorias_2024_04_03T14_03_18_602281_00_00_a57ed1cd6ebbb594d56257e078a83a4f
[2024-04-03T14:03:34.901+0000] {connection.py:269} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2024-04-03T14:03:34.925+0000] {base.py:83} INFO - Using connection ID 'google_cloud_default' for task execution.
[2024-04-03T14:03:34.941+0000] {bigquery.py:1516} INFO - Creating table
[2024-04-03T14:03:35.916+0000] {bigquery.py:1538} INFO - Table papyrus-technical-test.datalake_anyeli8.pedidos created successfully
[2024-04-03T14:03:35.949+0000] {taskinstance.py:2731} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 444, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 414, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 200, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/python.py", line 217, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/dags/habidag.py", line 105, in fetch_and_load_data_to_bigquery
    df[column["name"]] = df[column["name"]].astype(float)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/generic.py", line 6324, in astype
    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/internals/managers.py", line 451, in astype
    return self.apply(
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/internals/managers.py", line 352, in apply
    applied = getattr(b, f)(**kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/internals/blocks.py", line 511, in astype
    new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/dtypes/astype.py", line 242, in astype_array_safe
    new_values = astype_array(values, dtype, copy=copy)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/dtypes/astype.py", line 187, in astype_array
    values = _astype_nansafe(values, dtype, copy=copy)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/core/dtypes/astype.py", line 138, in _astype_nansafe
    return arr.astype(dtype, copy=True)
ValueError: could not convert string to float: '140,51'
[2024-04-03T14:03:36.004+0000] {taskinstance.py:1149} INFO - Marking task as UP_FOR_RETRY. dag_id=google_sheet_to_bigquery, task_id=fetch_and_load_data_to_bigquery, execution_date=20240403T140318, start_date=20240403T140322, end_date=20240403T140336
[2024-04-03T14:03:36.052+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 118 for task fetch_and_load_data_to_bigquery (could not convert string to float: '140,51'; 7672)
[2024-04-03T14:03:36.109+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2024-04-03T14:03:36.152+0000] {taskinstance.py:3312} INFO - 0 downstream tasks scheduled from follow-on schedule check
